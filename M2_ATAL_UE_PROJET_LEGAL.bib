
@inproceedings{shukla_legal_2022,
	address = {Online only},
	title = {Legal {Case} {Document} {Summarization}: {Extractive} and {Abstractive} {Methods} and their {Evaluation}},
	shorttitle = {Legal {Case} {Document} {Summarization}},
	url = {https://aclanthology.org/2022.aacl-main.77},
	abstract = {Summarization of legal case judgement documents is a challenging problem in Legal NLP. However, not much analyses exist on how different families of summarization models (e.g., extractive vs. abstractive) perform when applied to legal case documents. This question is particularly important since many recent transformer-based abstractive summarization models have restrictions on the number of input tokens, and legal documents are known to be very long. Also, it is an open question on how best to evaluate legal case document summarization systems. In this paper, we carry out extensive experiments with several extractive and abstractive summarization methods (both supervised and unsupervised) over three legal summarization datasets that we have developed. Our analyses, that includes evaluation by law practitioners, lead to several interesting insights on legal summarization in specific and long document summarization in general.},
	urldate = {2024-09-23},
	booktitle = {Proceedings of the 2nd {Conference} of the {Asia}-{Pacific} {Chapter} of the {Association} for {Computational} {Linguistics} and the 12th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Shukla, Abhay and Bhattacharya, Paheli and Poddar, Soham and Mukherjee, Rajdeep and Ghosh, Kripabandhu and Goyal, Pawan and Ghosh, Saptarshi},
	editor = {He, Yulan and Ji, Heng and Li, Sujian and Liu, Yang and Chang, Chua-Hui},
	month = nov,
	year = {2022},
	pages = {1048--1064},
	file = {Full Text PDF:/home/labicquette/Zotero/storage/DFP7QWL6/Shukla et al. - 2022 - Legal Case Document Summarization Extractive and Abstractive Methods and their Evaluation.pdf:application/pdf},
}

@inproceedings{shetty_automatic_2017,
	title = {Automatic extractive text summarization using {K}-means clustering},
	url = {https://ieeexplore.ieee.org/document/8284627},
	doi = {10.1109/ICEECCOT.2017.8284627},
	abstract = {The rise in the dimension of the World Wide Web has made an explosion of the amount of accessible information. As the textual data involves several instances of redundancy, omission of part of sentences or entire sentences is possible without altering the meaning of the document. Summarization of the text can informally be defined as the act of condensing the document from its original size without significantly compromising the semantics. For the purpose of generating an appropriate summary, the raw text is first pre-processed which involves - removing nonASCII characters and stop-words, tokenizing and stemming. Appropriate features are extracted from the data, tf-idf values for each word are computed and the entire pre-processed data is then transformed into a tf-idf matrix. Every sentence of the document will be represented as a vector in the dimensional space of the document's vocabulary. To obtain a concise summary, sentences are appropriately clustered based on the degree of separation of vectors in the Euclidean place. Association of sentences to a cluster using K-means method is totally based on cosine similarity. The count of the clusters is to be formed is predefined. As the number of clusters increase the accuracy of the summary increases. From each of the clusters the sentences which are informative are picked to form the final summary. Using recall and precision measures, the effectiveness of the summary is verified.},
	urldate = {2024-09-23},
	booktitle = {2017 {International} {Conference} on {Electrical}, {Electronics}, {Communication}, {Computer}, and {Optimization} {Techniques} ({ICEECCOT})},
	author = {Shetty, Krithi and Kallimani, Jagadish S.},
	month = dec,
	year = {2017},
	keywords = {Automata, Clustering, Cosine Similarity, Data mining, Feature extraction, Finite State Machine, Hidden Markov models, Lemmatization, Neural networks, Semantics, Sentence Extraction, Summary Generation, Vector Space Model, Vocabulary},
	pages = {1--9},
	file = {IEEE Xplore Abstract Record:/home/labicquette/Zotero/storage/5E79APD3/8284627.html:text/html;IEEE Xplore Full Text PDF:/home/labicquette/Zotero/storage/KVGNBF96/Shetty et Kallimani - 2017 - Automatic extractive text summarization using K-means clustering.pdf:application/pdf},
}

@article{fattah_hybrid_2014,
	title = {A hybrid machine learning model for multi-document summarization},
	volume = {40},
	issn = {1573-7497},
	url = {https://doi.org/10.1007/s10489-013-0490-0},
	doi = {10.1007/s10489-013-0490-0},
	abstract = {This work proposes an approach that uses statistical tools to improve content selection in multi-document automatic text summarization. The method uses a trainable summarizer, which takes into account several features: the similarity of words among sentences, the similarity of words among paragraphs, the text format, cue-phrases, a score related to the frequency of terms in the whole document, the title, sentence location and the occurrence of non-essential information. The effect of each of these sentence features on the summarization task is investigated. These features are then used in combination to construct text summarizer models based on a maximum entropy model, a naive-Bayes classifier, and a support vector machine. To produce the final summary, the three models are combined into a hybrid model that ranks the sentences in order of importance. The performance of this new method has been tested using the DUC 2002 data corpus. The effectiveness of this technique is measured using the ROUGE score, and the results are promising when compared with some existing techniques.},
	language = {en},
	number = {4},
	urldate = {2024-09-23},
	journal = {Applied Intelligence},
	author = {Fattah, Mohamed Abdel},
	month = jun,
	year = {2014},
	keywords = {Artificial Intelligence, Maximum entropy, Multi-document automatic summarization, Naive-Bayes, Support vector machine},
	pages = {592--600},
	file = {Full Text PDF:/home/labicquette/Zotero/storage/6THQT7XE/Fattah - 2014 - A hybrid machine learning model for multi-document summarization.pdf:application/pdf},
}

@article{cao_improving_2017,
	title = {Improving {Multi}-{Document} {Summarization} via {Text} {Classification}},
	volume = {31},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/10955},
	doi = {10.1609/aaai.v31i1.10955},
	abstract = {Developed so far, multi-document summarization has reached its bottleneck due to the lack of sufficient training data and diverse categories of documents. Text classification just makes up for these deficiencies.  In this paper, we propose a novel summarization system called TCSum, which leverages plentiful text classification data to improve the performance of multi-document summarization.  TCSum projects documents onto distributed representations which act as a bridge between text classification and summarization. It also utilizes the classification results to produce summaries of different styles. Extensive experiments on DUC generic multi-document summarization datasets show that, TCSum can achieve the state-of-the-art performance without using any hand-crafted features and has the capability to catch the variations of summary styles with respect to different text categories.},
	language = {en},
	number = {1},
	urldate = {2024-09-23},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Cao, Ziqiang and Li, Wenjie and Li, Sujian and Wei, Furu},
	month = feb,
	year = {2017},
	note = {Number: 1},
	keywords = {deep neural network},
	file = {Full Text PDF:/home/labicquette/Zotero/storage/4RAPKNN5/Cao et al. - 2017 - Improving Multi-Document Summarization via Text Classification.pdf:application/pdf},
}

@inproceedings{polsley_casesummarizer_2016,
	address = {Osaka, Japan},
	title = {{CaseSummarizer}: {A} {System} for {Automated} {Summarization} of {Legal} {Texts}},
	shorttitle = {{CaseSummarizer}},
	url = {https://aclanthology.org/C16-2054},
	abstract = {Attorneys, judges, and others in the justice system are constantly surrounded by large amounts of legal text, which can be difficult to manage across many cases. We present CaseSummarizer, a tool for automated text summarization of legal documents which uses standard summary methods based on word frequency augmented with additional domain-specific knowledge. Summaries are then provided through an informative interface with abbreviations, significance heat maps, and other flexible controls. It is evaluated using ROUGE and human scoring against several other summarization systems, including summary text and feedback provided by domain experts.},
	urldate = {2024-09-23},
	booktitle = {Proceedings of {COLING} 2016, the 26th {International} {Conference} on {Computational} {Linguistics}: {System} {Demonstrations}},
	publisher = {The COLING 2016 Organizing Committee},
	author = {Polsley, Seth and Jhunjhunwala, Pooja and Huang, Ruihong},
	editor = {Watanabe, Hideo},
	month = dec,
	year = {2016},
	pages = {258--262},
	file = {Full Text PDF:/home/labicquette/Zotero/storage/NUMVAMQH/Polsley et al. - 2016 - CaseSummarizer A System for Automated Summarization of Legal Texts.pdf:application/pdf},
}

@article{kanapala_text_2019,
	title = {Text summarization from legal documents: a survey},
	volume = {51},
	issn = {0269-2821, 1573-7462},
	shorttitle = {Text summarization from legal documents},
	url = {http://link.springer.com/10.1007/s10462-017-9566-2},
	doi = {10.1007/s10462-017-9566-2},
	language = {en},
	number = {3},
	urldate = {2024-09-23},
	journal = {Artificial Intelligence Review},
	author = {Kanapala, Ambedkar and Pal, Sukomal and Pamula, Rajendra},
	month = mar,
	year = {2019},
	pages = {371--402},
	file = {PDF:/home/labicquette/Zotero/storage/4XM9EP79/Kanapala et al. - 2019 - Text summarization from legal documents a survey.pdf:application/pdf},
}

@article{bauer_legal_nodate,
	title = {Legal {Extractive} {Summarization} of {U}.{S}. {Court} {Opinions}},
	abstract = {This paper tackles the task of legal extractive summarization using a dataset of 430K U.S. court opinions with key passages annotated. According to automated summary quality metrics, the reinforcement-learningbased MemSum model is best and even outperforms transformer-based models. In turn, the expert human evaluation shows that MemSum summaries effectively capture the key points of lengthy court opinions. Motivated by these results, we open-source our models to the general public. This represents progress toward democratizing law and making U.S. court opinions more accessible to the general public.},
	language = {en},
	author = {Bauer, Emmanuel},
	file = {PDF:/home/labicquette/Zotero/storage/UC39282H/Bauer - Legal Extractive Summarization of U.S. Court Opinions.pdf:application/pdf},
}

@book{nenkova_automatic_2011,
	title = {Automatic {Summarization}},
	volume = {5},
	abstract = {It has now been 50 years since the publication of Luhn's seminal paperon automatic summarization. During these years the practical need forautomatic summarization has become increasingly urgent and numer-ous papers have been published on the topic. As a result, it has become harder to find a single reference that gives an overview of past efforts or a complete view of summarization tasks and necessary system com-ponents. This article attempts to fill this void by providing a com-prehensive overview of research in summarization, including the more traditional efforts in sentence extraction as well as the most novel recent approaches for determining important content, for domain and genre specific summarization and for evaluation of summarization. We also discuss the challenges that remain open, in particular the need for lan-guage generation and deeper semantic understanding of language that would be necessary for future advances in the field.},
	author = {Nenkova, Ani and McKeown, Kathleen},
	month = jun,
	year = {2011},
	doi = {10.1561/1500000015},
	note = {Journal Abbreviation: Foundations and Trends in Information Retrieval
Publication Title: Foundations and Trends in Information Retrieval},
	file = {Full Text PDF:/home/labicquette/Zotero/storage/M3Y7GPJG/Nenkova et McKeown - 2011 - Automatic Summarization.pdf:application/pdf},
}

@phdthesis{le_neural_2020,
	type = {phdthesis},
	title = {Neural {Methods} for {Sentiment} {Analysis} and {Text} {Summarization}},
	url = {https://hal.univ-lorraine.fr/tel-02929745},
	abstract = {This thesis focuses on two Natural Language Processing tasks that require to extract semantic information from raw texts: Sentiment Analysis and Text Summarization. This dissertation discusses issues and seeks to improve neural models on both tasks, which have become the dominant paradigm in the past several years. Accordingly, this dissertation is composed of two parts: the first part (Neural Sentiment Analysis) deals with the computational study of people's opinions, sentiments, and the second part (Neural Text Summarization) tries to extract salient information from a complex sentence and rewrites it in a human-readable form. Neural Sentiment Analysis. Similar to computer vision, numerous deep convolutional neural networks have been adapted to sentiment analysis and text classification tasks. However, unlike the image domain, these studies are carried on different input data types and on different datasets, which makes it hard to know if a deep network is truly needed. In this thesis, we seek to find elements to address this question, i.e. whether neural networks must compute deep hierarchies of features for textual data in the same way as they do in vision. We thus propose a new adaptation of the deepest convolutional architecture (DenseNet) for text classification and study the importance of depth in convolutional models with different atom-levels (word or character) of input. We show that deep models indeed give better performances than shallow networks when the text input is represented as a sequence of characters. However, a simple shallow-and-wide network outperforms the deep DenseNet models with word inputs. Besides, to further improve sentiment classifiers and contextualize them, we propose to model them jointly with dialog acts, which are a factor of explanation and correlate with sentiments but are nevertheless often ignored. We have manually annotated both dialogues and sentiments on a Twitter-like social medium, and train a multi-task hierarchical recurrent network on joint sentiment and dialog act recognition. We show that transfer learning may be efficiently achieved between both tasks, and further analyze some specific correlations between sentiments and dialogues on social media. Neural Text Summarization. Detecting sentiments and opinions from large digital documents does not always enable users of such systems to take informed decisions, as other important semantic information is missing. People also need the main arguments and supporting reasons from the source documents to truly understand and interpret the document. To capture such information, we aim at making the neural text summarization models more explainable. We propose a model that has better explainability properties and is flexible enough to support various shallow syntactic parsing modules. More specifically, we linearize the syntactic tree into the form of overlapping text segments, which are then selected with reinforcement learning (RL) and regenerated into a compressed form. Hence, the proposed model is able to handle both extractive and abstractive summarization. Further, we observe that RL-based models are becoming increasingly ubiquitous for many text summarization tasks. We are interested in better understanding what types of information is taken into account by such models, and we propose to study this question from the syntactic perspective. We thus provide a detailed comparison of both RL-based and syntax-aware approaches and of their combination along several dimensions that relate to the perceived quality of the generated summaries such as number of repetitions, sentence length, distribution of part-of-speech tags, relevance and grammaticality. We show that when there is a resource constraint (computation and memory), it is wise to only train models with RL and without any syntactic information, as they provide nearly as good results as syntax-aware models with less parameters and faster training convergence.},
	language = {en},
	urldate = {2024-09-23},
	school = {Université de Lorraine},
	author = {Le, Thien-Hoa},
	month = may,
	year = {2020},
	file = {Full Text PDF:/home/labicquette/Zotero/storage/V5F8Q532/Le - 2020 - Neural Methods for Sentiment Analysis and Text Summarization.pdf:application/pdf},
}

@article{jain_summarization_2021,
	title = {Summarization of legal documents: {Where} are we now and the way forward},
	volume = {40},
	issn = {15740137},
	shorttitle = {Summarization of legal documents},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1574013721000289},
	doi = {10.1016/j.cosrev.2021.100388},
	abstract = {Due to huge amount of legal information availability on the internet, as well as other sources, it is important for the research community to do more extensive research on the area of legal text processing, which can help us make sense out of the vast amount of available data. This information growth has compelled the requirement to develop systems that can help legal professionals as well as ordinary citizens get relevant legal information with very little effort. In this survey paper, different text summarization techniques are surveyed, with a specific focus on legal document summarization, as this is one of the most important areas in the legal field, which can help with the quick understanding of legal documents. This paper starts with the general introduction to text summarization, following which various legal text summarization techniques are discussed. Various available tools are also described in this paper which is used for summarization of legal text. Two case studies are also presented in this work, where the automatic summarization of heterogeneous legal documents from two countries is considered. With the presented detailed review of the state of the art approaches, comparative analysis from the case studies and also discussions on several important research questions, this work is expected to provide a good starting point for researchers to perform a more in-depth exploration of the area of legal document summarization, more specifically with respect to the key future research directions identified in this work.},
	language = {en},
	urldate = {2024-09-23},
	journal = {Computer Science Review},
	author = {Jain, Deepali and Borah, Malaya Dutta and Biswas, Anupam},
	month = may,
	year = {2021},
	pages = {100388},
	file = {PDF:/home/labicquette/Zotero/storage/B28SLPBC/Jain et al. - 2021 - Summarization of legal documents Where are we now and the way forward.pdf:application/pdf},
}

@misc{allahyari_text_2017,
	title = {Text {Summarization} {Techniques}: {A} {Brief} {Survey}},
	shorttitle = {Text {Summarization} {Techniques}},
	url = {http://arxiv.org/abs/1707.02268},
	doi = {10.48550/arXiv.1707.02268},
	abstract = {In recent years, there has been a explosion in the amount of text data from a variety of sources. This volume of text is an invaluable source of information and knowledge which needs to be effectively summarized to be useful. In this review, the main approaches to automatic text summarization are described. We review the different processes for summarization and describe the effectiveness and shortcomings of the different methods.},
	urldate = {2024-09-23},
	publisher = {arXiv},
	author = {Allahyari, Mehdi and Pouriyeh, Seyedamin and Assefi, Mehdi and Safaei, Saeid and Trippe, Elizabeth D. and Gutierrez, Juan B. and Kochut, Krys},
	month = jul,
	year = {2017},
	note = {arXiv:1707.02268 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Some of references format have updated},
	file = {arXiv Fulltext PDF:/home/labicquette/Zotero/storage/TN884KGM/Allahyari et al. - 2017 - Text Summarization Techniques A Brief Survey.pdf:application/pdf;arXiv.org Snapshot:/home/labicquette/Zotero/storage/UPM9PUKS/1707.html:text/html},
}

@inproceedings{amplayo_entity_2018,
	address = {New Orleans, Louisiana},
	title = {Entity {Commonsense} {Representation} for {Neural} {Abstractive} {Summarization}},
	url = {https://aclanthology.org/N18-1064},
	doi = {10.18653/v1/N18-1064},
	abstract = {A major proportion of a text summary includes important entities found in the original text. These entities build up the topic of the summary. Moreover, they hold commonsense information once they are linked to a knowledge base. Based on these observations, this paper investigates the usage of linked entities to guide the decoder of a neural text summarizer to generate concise and better summaries. To this end, we leverage on an off-the-shelf entity linking system (ELS) to extract linked entities and propose Entity2Topic (E2T), a module easily attachable to a sequence-to-sequence model that transforms a list of entities into a vector representation of the topic of the summary. Current available ELS's are still not sufficiently effective, possibly introducing unresolved ambiguities and irrelevant entities. We resolve the imperfections of the ELS by (a) encoding entities with selective disambiguation, and (b) pooling entity vectors using firm attention. By applying E2T to a simple sequenceto-sequence model with attention mechanism as base model, we see significant improvements of the performance in the Gigaword (sentence to title) and CNN (long document to multi-sentence highlights) summarization datasets by at least 2 ROUGE points.},
	urldate = {2024-09-23},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Amplayo, Reinald Kim and Lim, Seonjae and Hwang, Seung-won},
	editor = {Walker, Marilyn and Ji, Heng and Stent, Amanda},
	month = jun,
	year = {2018},
	pages = {697--707},
	file = {Full Text PDF:/home/labicquette/Zotero/storage/5JVD26P7/Amplayo et al. - 2018 - Entity Commonsense Representation for Neural Abstractive Summarization.pdf:application/pdf},
}

@article{lin_abstractive_2019,
	title = {Abstractive {Summarization}: {A} {Survey} of the {State} of the {Art}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {Abstractive {Summarization}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/5056},
	doi = {10.1609/aaai.v33i01.33019815},
	abstract = {The focus of automatic text summarization research has exhibited a gradual shift from extractive methods to abstractive methods in recent years, owing in part to advances in neural methods. Originally developed for machine translation, neural methods provide a viable framework for obtaining an abstract representation of the meaning of an input text and generating informative, fluent, and human-like summaries. This paper surveys existing approaches to abstractive summarization, focusing on the recently developed neural approaches.},
	language = {en},
	number = {01},
	urldate = {2024-09-23},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Lin, Hui and Ng, Vincent},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {9815--9822},
	file = {Full Text PDF:/home/labicquette/Zotero/storage/YV8TI5ZS/Lin et Ng - 2019 - Abstractive Summarization A Survey of the State of the Art.pdf:application/pdf},
}

@incollection{azzopardi_comparative_2019,
	address = {Cham},
	title = {A {Comparative} {Study} of {Summarization} {Algorithms} {Applied} to {Legal} {Case} {Judgments}},
	volume = {11437},
	isbn = {978-3-030-15711-1 978-3-030-15712-8},
	url = {https://link.springer.com/10.1007/978-3-030-15712-8_27},
	language = {en},
	urldate = {2024-09-23},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Bhattacharya, Paheli and Hiware, Kaustubh and Rajgaria, Subham and Pochhi, Nilay and Ghosh, Kripabandhu and Ghosh, Saptarshi},
	editor = {Azzopardi, Leif and Stein, Benno and Fuhr, Norbert and Mayr, Philipp and Hauff, Claudia and Hiemstra, Djoerd},
	year = {2019},
	doi = {10.1007/978-3-030-15712-8_27},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {413--428},
}

@misc{beltagy_longformer_2020,
	title = {Longformer: {The} {Long}-{Document} {Transformer}},
	shorttitle = {Longformer},
	url = {http://arxiv.org/abs/2004.05150},
	abstract = {Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.},
	language = {en},
	urldate = {2024-09-24},
	publisher = {arXiv},
	author = {Beltagy, Iz and Peters, Matthew E. and Cohan, Arman},
	month = dec,
	year = {2020},
	note = {arXiv:2004.05150 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Version 2 introduces the Longformer-Encoder-Decoder (LED) model},
	file = {PDF:/home/labicquette/Zotero/storage/QM7HBZXR/Beltagy et al. - 2020 - Longformer The Long-Document Transformer.pdf:application/pdf},
}

@misc{lewis_bart_2019,
	title = {{BART}: {Denoising} {Sequence}-to-{Sequence} {Pre}-training for {Natural} {Language} {Generation}, {Translation}, and {Comprehension}},
	shorttitle = {{BART}},
	url = {http://arxiv.org/abs/1910.13461},
	abstract = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, ﬁnding the best performance by both randomly shufﬂing the order of the original sentences and using a novel in-ﬁlling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when ﬁne tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most inﬂuence end-task performance.},
	language = {en},
	urldate = {2024-09-24},
	publisher = {arXiv},
	author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
	month = oct,
	year = {2019},
	note = {arXiv:1910.13461 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {PDF:/home/labicquette/Zotero/storage/TWYCUVQ9/Lewis et al. - 2019 - BART Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and.pdf:application/pdf},
}

@misc{noauthor_law-aisummarization_2024,
	title = {Law-{AI}/summarization},
	url = {https://github.com/Law-AI/summarization},
	abstract = {Implementation of different summarization algorithms applied to legal case judgements.},
	urldate = {2024-09-24},
	publisher = {Law and AI, IIT Kharagpur},
	month = sep,
	year = {2024},
	note = {original-date: 2018-12-17T08:29:13Z},
	keywords = {artificial-intelligence, legal-texts, python, summarization},
}

@misc{noauthor_supreme_nodate,
	title = {Supreme {Court} of {India}},
	url = {http://www.liiofindia.org/in/cases/cen/INSC/},
	urldate = {2024-09-24},
	file = {Supreme Court of India:/home/labicquette/Zotero/storage/MXFHXB6G/INSC.html:text/html},
}

@inproceedings{darrin_cosmic_2024,
	address = {Bangkok, Thailand},
	title = {{COSMIC}: {Mutual} {Information} for {Task}-{Agnostic} {Summarization} {Evaluation}},
	shorttitle = {{COSMIC}},
	url = {https://aclanthology.org/2024.acl-long.686},
	doi = {10.18653/v1/2024.acl-long.686},
	abstract = {Assessing the quality of summarizers poses significant challenges—gold summaries are hard to obtain and their suitability depends on the use context of the summarization system. Who is the user of the system, and what do they intend to do with the summary? In response, we propose a novel task-oriented evaluation approach that assesses summarizers based on their capacity to produce summaries while preserving task outcomes. We theoretically establish both a lower and upper bound on the expected error rate of these tasks, which depends on the mutual information between source texts and generated summaries. We introduce COSMIC, a practical implementation of this metric, and demonstrate its strong correlation with human judgment-based metrics, as well as its effectiveness in predicting downstream task performance. Comparative analyses against established metrics like BERTScore and ROUGE highlight the competitive performance of COSMIC.},
	urldate = {2024-09-24},
	booktitle = {Proceedings of the 62nd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Darrin, Maxime and Formont, Philippe and Cheung, Jackie and Piantanida, Pablo},
	editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
	month = aug,
	year = {2024},
	pages = {12696--12717},
	file = {Full Text PDF:/home/labicquette/Zotero/storage/AZ6IQP5A/Darrin et al. - 2024 - COSMIC Mutual Information for Task-Agnostic Summarization Evaluation.pdf:application/pdf},
}

@misc{court_decided_nodate,
	title = {Decided cases - {The} {Supreme} {Court}},
	url = {https://www.supremecourt.uk/decided-cases/},
	abstract = {Decided cases},
	language = {ENG},
	urldate = {2024-09-24},
	author = {Court, The Supreme},
	file = {Snapshot:/home/labicquette/Zotero/storage/8F8GG2KJ/decided-cases.html:text/html},
}

@misc{noauthor_py-rouge_nodate,
	title = {py-rouge: {Full} {Python} implementation of the {ROUGE} metric, producing same results as in the official perl implementation.},
	shorttitle = {py-rouge},
	url = {https://github.com/Diego999/py-rouge},
	urldate = {2024-09-24},
	file = {Snapshot:/home/labicquette/Zotero/storage/J9AQ6KD9/py-rouge.html:text/html},
}

@misc{noauthor_nsi319legal-led-base-16384_nodate,
	title = {nsi319/legal-led-base-16384 · {Hugging} {Face}},
	url = {https://huggingface.co/nsi319/legal-led-base-16384},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-09-24},
	file = {Snapshot:/home/labicquette/Zotero/storage/WUPEW42Y/legal-led-base-16384.html:text/html},
}

@misc{noauthor_nsi319legal-pegasus_nodate,
	title = {nsi319/legal-pegasus · {Hugging} {Face}},
	url = {https://huggingface.co/nsi319/legal-pegasus},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-09-24},
	file = {Snapshot:/home/labicquette/Zotero/storage/SBAZCCK9/legal-pegasus.html:text/html},
}

@misc{noauthor_facebookbart-large_nodate,
	title = {facebook/bart-large · {Hugging} {Face}},
	url = {https://huggingface.co/facebook/bart-large},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-09-24},
	file = {Snapshot:/home/labicquette/Zotero/storage/UKWF4I5T/bart-large.html:text/html},
}

@misc{noauthor_bert-score_nodate,
	title = {bert-score: {PyTorch} implementation of {BERT} score},
	copyright = {OSI Approved :: MIT License},
	shorttitle = {bert-score},
	url = {https://github.com/Tiiiger/bert_score},
	urldate = {2024-09-24},
	keywords = {BERT,, deep,, google,, learning,, metric, NLP,, Scientific/Engineering - Artificial Intelligence},
	file = {Snapshot:/home/labicquette/Zotero/storage/FFMW8VLE/bert-score.html:text/html},
}

@inproceedings{deutsch_limitations_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {On the {Limitations} of {Reference}-{Free} {Evaluations} of {Generated} {Text}},
	url = {https://aclanthology.org/2022.emnlp-main.753},
	doi = {10.18653/v1/2022.emnlp-main.753},
	abstract = {There is significant interest in developing evaluation metrics which accurately estimate the quality of generated text without the aid of a human-written reference text, which can be time consuming and expensive to collect or entirely unavailable in online applications. However, in this work, we demonstrate that these reference-free metrics are inherently biased and limited in their ability to evaluate generated text, and we argue that they should not be used to measure progress on tasks like machine translation or summarization. We show how reference-free metrics are equivalent to using one generation model to evaluate another, which has several limitations: (1) the metrics can be optimized at test time to find the approximate best-possible output, (2) they are inherently biased toward models which are more similar to their own, and (3) they can be biased against higher-quality outputs, including those written by humans. Therefore, we recommend that reference-free metrics should be used as diagnostic tools for analyzing and understanding model behavior instead of measures of how well models perform a task, in which the goal is to achieve as high of a score as possible.},
	urldate = {2024-09-24},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Deutsch, Daniel and Dror, Rotem and Roth, Dan},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	pages = {10960--10977},
	file = {Full Text PDF:/home/labicquette/Zotero/storage/QU7PGL7K/Deutsch et al. - 2022 - On the Limitations of Reference-Free Evaluations of Generated Text.pdf:application/pdf},
}

@misc{noauthor_pile--lawpile--law_2023,
	title = {pile-of-law/pile-of-law · {Datasets} at {Hugging} {Face}},
	url = {https://huggingface.co/datasets/pile-of-law/pile-of-law},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-09-24},
	month = jun,
	year = {2023},
	file = {Snapshot:/home/labicquette/Zotero/storage/SUE7FMKT/pile-of-law.html:text/html},
}

@article{guha_legalbench_2023,
	title = {{LegalBench}: {A} {Collaboratively} {Built} {Benchmark} for {Measuring} {Legal} {Reasoning} in {Large} {Language} {Models}},
	volume = {36},
	shorttitle = {{LegalBench}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/89e44582fd28ddfea1ea4dcb0ebbf4b0-Abstract-Datasets_and_Benchmarks.html},
	language = {en},
	urldate = {2024-09-25},
	journal = {Advances in Neural Information Processing Systems},
	author = {Guha, Neel and Nyarko, Julian and Ho, Daniel and Ré, Christopher and Chilton, Adam and K, Aditya and Chohlas-Wood, Alex and Peters, Austin and Waldon, Brandon and Rockmore, Daniel and Zambrano, Diego and Talisman, Dmitry and Hoque, Enam and Surani, Faiz and Fagan, Frank and Sarfaty, Galit and Dickinson, Gregory and Porat, Haggai and Hegland, Jason and Wu, Jessica and Nudell, Joe and Niklaus, Joel and Nay, John and Choi, Jonathan and Tobia, Kevin and Hagan, Margaret and Ma, Megan and Livermore, Michael and Rasumov-Rahe, Nikon and Holzenberger, Nils and Kolt, Noam and Henderson, Peter and Rehaag, Sean and Goel, Sharad and Gao, Shang and Williams, Spencer and Gandhi, Sunny and Zur, Tom and Iyer, Varun and Li, Zehua},
	month = dec,
	year = {2023},
	pages = {44123--44279},
	file = {Full Text PDF:/home/labicquette/Zotero/storage/WCP797T4/Guha et al. - 2023 - LegalBench A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models.pdf:application/pdf},
}

@article{hendrycks_cuad_2021,
	title = {{CUAD}: {An} {Expert}-{Annotated} {NLP} {Dataset} for {Legal} {Contract} {Review}},
	volume = {1},
	shorttitle = {{CUAD}},
	url = {https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/hash/6ea9ab1baa0efb9e19094440c317e21b-Abstract-round1.html},
	language = {en},
	urldate = {2024-09-25},
	journal = {Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks},
	author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},
	month = dec,
	year = {2021},
	file = {Full Text PDF:/home/labicquette/Zotero/storage/YQG5MFHP/Hendrycks et al. - 2021 - CUAD An Expert-Annotated NLP Dataset for Legal Contract Review.pdf:application/pdf},
}

@inproceedings{wang_maud_2023,
	address = {Singapore},
	title = {{MAUD}: {An} {Expert}-{Annotated} {Legal} {NLP} {Dataset} for {Merger} {Agreement} {Understanding}},
	shorttitle = {{MAUD}},
	url = {https://aclanthology.org/2023.emnlp-main.1019},
	doi = {10.18653/v1/2023.emnlp-main.1019},
	abstract = {Reading comprehension of legal text can be a particularly challenging task due to the length and complexity of legal clauses and a shortage of expert-annotated datasets. To address this challenge, we introduce the Merger Agreement Understanding Dataset (MAUD), an expert-annotated reading comprehension dataset based on the American Bar Association's 2021 Public Target Deal Points Study, with over 39,000 examples and over 47,000 total annotations. Our fine-tuned Transformer baselines show promising results, with models performing well above random on most questions. However, on a large subset of questions, there is still room for significant improvement. As the only expert-annotated merger agreement dataset, MAUD is valuable as a benchmark for both the legal profession and the NLP community.},
	urldate = {2024-09-25},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Steven and Scardigli, Antoine and Tang, Leonard and Chen, Wei and Levkin, Dmitry and Chen, Anya and Ball, Spencer and Woodside, Thomas and Zhang, Oliver and Hendrycks, Dan},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {16369--16382},
	file = {Full Text PDF:/home/labicquette/Zotero/storage/A4ITNFWZ/Wang et al. - 2023 - MAUD An Expert-Annotated Legal NLP Dataset for Merger Agreement Understanding.pdf:application/pdf},
}

@article{shen_multi-lexsum_2022,
	title = {Multi-{LexSum}: {Real}-world {Summaries} of {Civil} {Rights} {Lawsuits} at {Multiple} {Granularities}},
	volume = {35},
	shorttitle = {Multi-{LexSum}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/552ef803bef9368c29e53c167de34b55-Abstract-Datasets_and_Benchmarks.html},
	language = {en},
	urldate = {2024-09-25},
	journal = {Advances in Neural Information Processing Systems},
	author = {Shen, Zejiang and Lo, Kyle and Yu, Lauren and Dahlberg, Nathan and Schlanger, Margo and Downey, Doug},
	month = dec,
	year = {2022},
	pages = {13158--13173},
	file = {Full Text PDF:/home/labicquette/Zotero/storage/CGR6J8JD/Shen et al. - 2022 - Multi-LexSum Real-world Summaries of Civil Rights Lawsuits at Multiple Granularities.pdf:application/pdf},
}

@misc{mamakas_processing_2022,
	title = {Processing {Long} {Legal} {Documents} with {Pre}-trained {Transformers}: {Modding} {LegalBERT} and {Longformer}},
	shorttitle = {Processing {Long} {Legal} {Documents} with {Pre}-trained {Transformers}},
	url = {http://arxiv.org/abs/2211.00974},
	doi = {10.48550/arXiv.2211.00974},
	abstract = {Pre-trained Transformers currently dominate most NLP tasks. They impose, however, limits on the maximum input length (512 sub-words in BERT), which are too restrictive in the legal domain. Even sparse-attention models, such as Longformer and BigBird, which increase the maximum input length to 4,096 sub-words, severely truncate texts in three of the six datasets of LexGLUE. Simpler linear classifiers with TF-IDF features can handle texts of any length, require far less resources to train and deploy, but are usually outperformed by pre-trained Transformers. We explore two directions to cope with long legal texts: (i) modifying a Longformer warm-started from LegalBERT to handle even longer texts (up to 8,192 sub-words), and (ii) modifying LegalBERT to use TF-IDF representations. The first approach is the best in terms of performance, surpassing a hierarchical version of LegalBERT, which was the previous state of the art in LexGLUE. The second approach leads to computationally more efficient models at the expense of lower performance, but the resulting models still outperform overall a linear SVM with TF-IDF features in long legal document classification.},
	urldate = {2024-09-25},
	publisher = {arXiv},
	author = {Mamakas, Dimitris and Tsotsi, Petros and Androutsopoulos, Ion and Chalkidis, Ilias},
	month = nov,
	year = {2022},
	note = {arXiv:2211.00974 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 9 pages, long paper at NLLP Workshop 2022 proceedings},
	file = {arXiv Fulltext PDF:/home/labicquette/Zotero/storage/X7V4XAJK/Mamakas et al. - 2022 - Processing Long Legal Documents with Pre-trained Transformers Modding LegalBERT and Longformer.pdf:application/pdf;arXiv.org Snapshot:/home/labicquette/Zotero/storage/PYBLTKYW/2211.html:text/html},
}

@inproceedings{li_dont_2023,
	address = {New York, NY, USA},
	series = {{ICAIL} '23},
	title = {Don't {Use} a {Cannon} to {Kill} a {Fly}: {An} {Efficient} {Cascading} {Pipeline} for {Long} {Documents}},
	isbn = {9798400701979},
	shorttitle = {Don't {Use} a {Cannon} to {Kill} a {Fly}},
	url = {https://doi.org/10.1145/3594536.3595142},
	doi = {10.1145/3594536.3595142},
	abstract = {The computational cost of transformer-based models has a quadratic dependence on the length of the input sequence. This makes it challenging to deploy these models in domains in which long documents are especially lengthy, such as the legal domain. To address this issue, we propose a three-stage cascading approach for long document classification. We begin by filtering out likely irrelevant information with a lightweight logistic regression model before passing the more challenging inputs to the transformer-based model. We evaluate our approach using CUAD, a legal dataset with 510 manually-annotated, long contracts. We find that the cascading approach reduces training time by up to 80\% while improving baseline performance. We hypothesize that the gains in performance stem from localizing the classification task of the transformer model to particularly difficult examples.},
	urldate = {2024-09-25},
	booktitle = {Proceedings of the {Nineteenth} {International} {Conference} on {Artificial} {Intelligence} and {Law}},
	publisher = {Association for Computing Machinery},
	author = {Li, Zehua and Guha, Neel and Nyarko, Julian},
	month = sep,
	year = {2023},
	pages = {141--147},
}

@inproceedings{chalkidis_lexglue_2022,
	address = {Dublin, Ireland},
	title = {{LexGLUE}: {A} {Benchmark} {Dataset} for {Legal} {Language} {Understanding} in {English}},
	shorttitle = {{LexGLUE}},
	url = {https://aclanthology.org/2022.acl-long.297},
	doi = {10.18653/v1/2022.acl-long.297},
	abstract = {Laws and their interpretations, legal arguments and agreements are typically expressed in writing, leading to the production of vast corpora of legal text. Their analysis, which is at the center of legal practice, becomes increasingly elaborate as these collections grow in size. Natural language understanding (NLU) technologies can be a valuable tool to support legal practitioners in these endeavors. Their usefulness, however, largely depends on whether current state-of-the-art models can generalize across various tasks in the legal domain. To answer this currently open question, we introduce the Legal General Language Understanding Evaluation (LexGLUE) benchmark, a collection of datasets for evaluating model performance across a diverse set of legal NLU tasks in a standardized way. We also provide an evaluation and analysis of several generic and legal-oriented models demonstrating that the latter consistently offer performance improvements across multiple tasks.},
	urldate = {2024-09-25},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Chalkidis, Ilias and Jana, Abhik and Hartung, Dirk and Bommarito, Michael and Androutsopoulos, Ion and Katz, Daniel and Aletras, Nikolaos},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	pages = {4310--4330},
	file = {Full Text PDF:/home/labicquette/Zotero/storage/3PM4DMQZ/Chalkidis et al. - 2022 - LexGLUE A Benchmark Dataset for Legal Language Understanding in English.pdf:application/pdf},
}

@inproceedings{zheng_when_2021,
	address = {New York, NY, USA},
	series = {{ICAIL} '21},
	title = {When does pretraining help? assessing self-supervised learning for law and the {CaseHOLD} dataset of 53,000+ legal holdings},
	isbn = {978-1-4503-8526-8},
	shorttitle = {When does pretraining help?},
	url = {https://dl.acm.org/doi/10.1145/3462757.3466088},
	doi = {10.1145/3462757.3466088},
	abstract = {While self-supervised learning has made rapid advances in natural language processing, it remains unclear when researchers should engage in resource-intensive domain-specific pretraining (domain pretraining). The law, puzzlingly, has yielded few documented instances of substantial gains to domain pretraining in spite of the fact that legal language is widely seen to be unique. We hypothesize that these existing results stem from the fact that existing legal NLP tasks are too easy and fail to meet conditions for when domain pretraining can help. To address this, we first present CaseHOLD (Case \&lt;u\&gt;H\&lt;/u\&gt;oldings \&lt;u\&gt;O\&lt;/u\&gt;n \&lt;u\&gt;L\&lt;/u\&gt;egal \&lt;u\&gt;D\&lt;/u\&gt;ecisions), a new dataset comprised of over 53,000+ multiple choice questions to identify the relevant holding of a cited case. This dataset presents a fundamental task to lawyers and is both legally meaningful and difficult from an NLP perspective (F1 of 0.4 with a BiLSTM baseline). Second, we assess performance gains on CaseHOLD and existing legal NLP datasets. While a Transformer architecture (BERT) pretrained on a general corpus (Google Books and Wikipedia) improves performance, domain pretraining (on a corpus of ≈3.5M decisions across all courts in the U.S. that is larger than BERT's) with a custom legal vocabulary exhibits the most substantial performance gains with CaseHOLD (gain of 7.2\% on F1, representing a 12\% improvement on BERT) and consistent performance gains across two other legal tasks. Third, we show that domain pretraining may be warranted when the task exhibits sufficient similarity to the pretraining corpus: the level of performance increase in three legal tasks was directly tied to the domain specificity of the task. Our findings inform when researchers should engage in resource-intensive pretraining and show that Transformer-based architectures, too, learn embeddings suggestive of distinct legal language.},
	urldate = {2024-09-25},
	booktitle = {Proceedings of the {Eighteenth} {International} {Conference} on {Artificial} {Intelligence} and {Law}},
	publisher = {Association for Computing Machinery},
	author = {Zheng, Lucia and Guha, Neel and Anderson, Brandon R. and Henderson, Peter and Ho, Daniel E.},
	month = jul,
	year = {2021},
	pages = {159--168},
	file = {Full Text PDF:/home/labicquette/Zotero/storage/4CGR53FN/Zheng et al. - 2021 - When does pretraining help assessing self-supervised learning for law and the CaseHOLD dataset of 5.pdf:application/pdf},
}

@inproceedings{zhao_summpip_2020,
	address = {Virtual Event China},
	title = {{SummPip}: {Unsupervised} {Multi}-{Document} {Summarization} with {Sentence} {Graph} {Compression}},
	isbn = {978-1-4503-8016-4},
	shorttitle = {{SummPip}},
	url = {https://dl.acm.org/doi/10.1145/3397271.3401327},
	doi = {10.1145/3397271.3401327},
	language = {en},
	urldate = {2024-09-25},
	booktitle = {Proceedings of the 43rd {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Zhao, Jinming and Liu, Ming and Gao, Longxiang and Jin, Yuan and Du, Lan and Zhao, He and Zhang, He and Haffari, Gholamreza},
	month = jul,
	year = {2020},
	pages = {1949--1952},
	file = {Submitted Version:/home/labicquette/Zotero/storage/72P8C48K/Zhao et al. - 2020 - SummPip Unsupervised Multi-Document Summarization with Sentence Graph Compression.pdf:application/pdf},
}

@article{yang_integrated_2018,
	title = {An {Integrated} {Graph} {Model} for {Document} {Summarization}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2078-2489},
	url = {https://www.mdpi.com/2078-2489/9/9/232},
	doi = {10.3390/info9090232},
	abstract = {Extractive summarization aims to produce a concise version of a document by extracting information-rich sentences from the original texts. The graph-based model is an effective and efficient approach to rank sentences since it is simple and easy to use. However, its performance depends heavily on good text representation. In this paper, an integrated graph model (iGraph) for extractive text summarization is proposed. An enhanced embedding model is used to detect the inherent semantic properties at the word level, bigram level and trigram level. Words with part-of-speech (POS) tags, bigrams and trigrams were extracted to train the embedding models. Based on the enhanced embedding vectors, the similarity values between the sentences were calculated in three perspectives. The sentences in the document were treated as vertexes and the similarity between them as edges. As a result, three different types of semantic graphs were obtained for every document, with the same nodes and different edges. These three graphs were integrated into one enriched semantic graph in a naive Bayesian fashion. After that, TextRank, which is a graph-based ranking algorithm, was applied to rank the sentences, before the top scored sentences were selected for the summary according to the compression rate. Evaluated on the DUC 2002 and DUC 2004 datasets, our proposed method shows competitive performance compared to the state-of-the-art methods.},
	language = {en},
	number = {9},
	urldate = {2024-09-25},
	journal = {Information},
	author = {Yang, Kang and Al-Sabahi, Kamal and Xiang, Yanmin and Zhang, Zuping},
	month = sep,
	year = {2018},
	note = {Number: 9
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {document summarization, graph integration, TextRank, word embedding},
	pages = {232},
	file = {Full Text PDF:/home/labicquette/Zotero/storage/BXPY8DX2/Yang et al. - 2018 - An Integrated Graph Model for Document Summarization.pdf:application/pdf},
}

@misc{goncalves_automatic_2021,
	title = {Automatic {Text} {Summarization} with {Machine} {Learning} — {An} overview},
	url = {https://medium.com/luisfredgs/automatic-text-summarization-with-machine-learning-an-overview-68ded5717a25},
	abstract = {Summarization is the task of condensing a piece of text to a shorter version, reducing the size of the initial text while at the same time…},
	language = {en},
	urldate = {2024-09-26},
	journal = {luisfredgs},
	author = {Gonçalves, Luís},
	month = sep,
	year = {2021},
	file = {Snapshot:/home/labicquette/Zotero/storage/X6DJ8YJX/automatic-text-summarization-with-machine-learning-an-overview-68ded5717a25.html:text/html},
}

@article{ozsoy_text_2011,
	title = {Text summarization using {Latent} {Semantic} {Analysis}},
	volume = {37},
	issn = {0165-5515},
	url = {https://doi.org/10.1177/0165551511408848},
	doi = {10.1177/0165551511408848},
	abstract = {Text summarization solves the problem of presenting the information needed by a user in a compact form. There are different approaches to creating well-formed summaries. One of the newest methods is the Latent Semantic Analysis (LSA). In this paper, different LSA-based summarization algorithms are explained, two of which are proposed by the authors of this paper. The algorithms are evaluated on Turkish and English documents, and their performances are compared using their ROUGE scores. One of our algorithms produces the best scores and both algorithms perform equally well on Turkish and English document sets.},
	language = {en},
	number = {4},
	urldate = {2024-09-26},
	journal = {Journal of Information Science},
	author = {Ozsoy, Makbule Gulcin and Alpaslan, Ferda Nur and Cicekli, Ilyas},
	month = aug,
	year = {2011},
	note = {Publisher: SAGE Publications Ltd},
	pages = {405--417},
	file = {405-417JIS-408848:/home/labicquette/Zotero/storage/GIUP7HWM/405-417JIS-408848.pdf:application/pdf;Version soumise:/home/labicquette/Zotero/storage/IWHMPVJX/Ozsoy et al. - 2011 - Text summarization using Latent Semantic Analysis.pdf:application/pdf},
}

@misc{goncalves_luisfredgslsa-text-summarization_2024,
	title = {luisfredgs/{LSA}-{Text}-{Summarization}},
	url = {https://github.com/luisfredgs/LSA-Text-Summarization},
	urldate = {2024-09-26},
	author = {Gonçalves, Luís},
	month = may,
	year = {2024},
	note = {original-date: 2020-04-10T22:58:06Z},
}

@misc{noauthor_how_nodate,
	title = {How to {Summarize} {Text} {Using} {Machine} {Learning} {Models}},
	url = {https://www.edlitera.com/blog/posts/text-summarization-nlp-how-to},
	abstract = {The techniques shown here have wide applications.},
	urldate = {2024-09-26},
	journal = {Edlitera},
	file = {Snapshot:/home/labicquette/Zotero/storage/2XMJ6ZAP/text-summarization-nlp-how-to.html:text/html},
}

@article{mihalcea_textrank_nodate,
	title = {{TextRank}: {Bringing} {Order} into {Texts}},
	abstract = {In this paper, we introduce TextRank – a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications. In particular, we propose two innovative unsupervised methods for keyword and sentence extraction, and show that the results obtained compare favorably with previously published results on established benchmarks.},
	language = {en},
	author = {Mihalcea, Rada and Tarau, Paul},
	file = {PDF:/home/labicquette/Zotero/storage/66AI7ICV/Mihalcea et Tarau - TextRank Bringing Order into Texts.pdf:application/pdf},
}

@misc{zhang_pegasus_2020,
	title = {{PEGASUS}: {Pre}-training with {Extracted} {Gap}-sentences for {Abstractive} {Summarization}},
	shorttitle = {{PEGASUS}},
	url = {http://arxiv.org/abs/1912.08777},
	abstract = {Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets.},
	language = {en},
	urldate = {2024-09-26},
	publisher = {arXiv},
	author = {Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter J.},
	month = jul,
	year = {2020},
	note = {arXiv:1912.08777 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Added results from mixed+stochastic model, test-set overlapping analysis; Code link added; Accepted for ICML 2020. arXiv admin note: text overlap with arXiv:1605.06560, arXiv:1205.2395, arXiv:0902.4351, arXiv:1610.09932, arXiv:nucl-ex/0512029 by other authors},
	file = {PDF:/home/labicquette/Zotero/storage/9FUZEBD2/Zhang et al. - 2020 - PEGASUS Pre-training with Extracted Gap-sentences for Abstractive Summarization.pdf:application/pdf},
}

@misc{noauthor_automatic_nodate,
	title = {Automatic {Text} {Summarization} {System} {Using} {Transformers}},
	url = {https://www.neurond.com/blog/automatic-text-summarization-system-using-transformers},
	abstract = {Click here to learn how to build a text summarization system using the BART transformer and PEGASUS models!},
	language = {en},
	urldate = {2024-09-26},
}

@misc{kumar_techniques_2019,
	title = {Techniques for {Extractive} and {Abstractive} text summarization},
	url = {https://medium.com/@devilchauhan0/techniques-for-extractive-and-abstractive-text-summarization-6ed44a5465f6},
	abstract = {Authors: Ojas Mithbavkar and Ankush Chau},
	language = {en},
	urldate = {2024-09-26},
	journal = {Medium},
	author = {Kumar, Ankush},
	month = nov,
	year = {2019},
}

@inproceedings{gong_generic_2001,
	address = {New York, NY, USA},
	series = {{SIGIR} '01},
	title = {Generic text summarization using relevance measure and latent semantic analysis},
	isbn = {978-1-58113-331-8},
	url = {https://doi.org/10.1145/383952.383955},
	doi = {10.1145/383952.383955},
	abstract = {In this paper, we propose two generic text summarization methods that create text summaries by ranking and extracting sentences from the original documents. The first method uses standard IR methods to rank sentence relevances, while the second method uses the latent semantic analysis technique to identify semantically important sentences, for summary creations. Both methods strive to select sentences that are highly ranked and different from each other. This is an attempt to create a summary with a wider coverage of the document's main content and less redundancy. Performance evaluations on the two summarization methods are conducted by comparing their summarization outputs with the manual summaries generated by three independent human evaluators. The evaluations also study the influence of different VSM weighting schemes on the text summarization performances. Finally, the causes of the large disparities in the evaluators' manual summarization results are investigated, and discussions on human text summarization patterns are presented.},
	urldate = {2024-09-26},
	booktitle = {Proceedings of the 24th annual international {ACM} {SIGIR} conference on {Research} and development in information retrieval},
	publisher = {Association for Computing Machinery},
	author = {Gong, Yihong and Liu, Xin},
	month = sep,
	year = {2001},
	pages = {19--25},
	file = {Version soumise:/home/labicquette/Zotero/storage/XRPQRHSY/Gong et Liu - 2001 - Generic text summarization using relevance measure and latent semantic analysis.pdf:application/pdf},
}

@misc{sharma_text_2024,
	title = {Text {Summarization} with {BART} {Model}},
	url = {https://medium.com/@sandyeep70/demystifying-text-summarization-with-deep-learning-ce08d99eda97},
	abstract = {Introduction},
	language = {en},
	urldate = {2024-09-27},
	journal = {Medium},
	author = {Sharma, Sandeep},
	month = feb,
	year = {2024},
}

@misc{zhao_hpzhaosummarunner_2024,
	title = {hpzhao/{SummaRuNNer}},
	copyright = {MIT},
	url = {https://github.com/hpzhao/SummaRuNNer},
	abstract = {The PyTorch Implementation of SummaRuNNer},
	urldate = {2024-09-26},
	author = {Zhao, Huaipeng},
	month = may,
	year = {2024},
	note = {original-date: 2017-07-22T08:06:42Z},
	keywords = {extractive-summarization, pytorch, pytorch-implmention, summarunner, summary},
}

@misc{saurav_6_2023,
	title = {6 {Useful} {Text} {Summarization} {Algorithm} in {Python}},
	url = {https://medium.com/@sarowar.saurav10/6-useful-text-summarization-algorithm-in-python-dfc8a9d33074},
	abstract = {Are you fascinated by the magic of Python algorithms that can distill vast oceans of text into concise, insightful summaries? 📚✂️ Get…},
	language = {en},
	urldate = {2024-09-27},
	journal = {Medium},
	author = {Saurav, Sarowar Jahan},
	month = oct,
	year = {2023},
	file = {Snapshot:/home/labicquette/Zotero/storage/5IECYSKM/6-useful-text-summarization-algorithm-in-python-dfc8a9d33074.html:text/html},
}

@misc{goncalves_luisfredgslsa-text-summarization_2024-1,
	title = {luisfredgs/{LSA}-{Text}-{Summarization}},
	url = {https://github.com/luisfredgs/LSA-Text-Summarization},
	urldate = {2024-09-27},
	author = {Gonçalves, Luís},
	month = may,
	year = {2024},
	note = {original-date: 2020-04-10T22:58:06Z},
}

@misc{noauthor_document_nodate,
	title = {{DOCUMENT} {SUMMARIZATION}-{Using} {LSA}},
	url = {https://kaggle.com/code/jurk06/document-summarization-using-lsa},
	abstract = {Explore and run machine learning code with Kaggle Notebooks {\textbar} Using data from No attached data sources},
	language = {en},
	urldate = {2024-09-27},
	file = {Snapshot:/home/labicquette/Zotero/storage/9GZELWCL/document-summarization-using-lsa.html:text/html},
}

@misc{noauthor_pegasussummarizationpegasus_nodate,
	title = {{PegasusSummarization}/{Pegasus} {Tutorial}.ipynb at main · nicknochnack/{PegasusSummarization}},
	url = {https://github.com/nicknochnack/PegasusSummarization/blob/main/Pegasus%20Tutorial.ipynb},
	urldate = {2024-09-27},
	file = {PegasusSummarization/Pegasus Tutorial.ipynb at main · nicknochnack/PegasusSummarization:/home/labicquette/Zotero/storage/EU2PGBLE/Pegasus Tutorial.html:text/html},
}

@misc{renotte_nicknochnackpegasussummarization_2024,
	title = {nicknochnack/{PegasusSummarization}},
	url = {https://github.com/nicknochnack/PegasusSummarization},
	abstract = {A quick walkthrough demonstrating how to use Pegasus for summarization.},
	urldate = {2024-09-27},
	author = {Renotte, Nicholas},
	month = jul,
	year = {2024},
	note = {original-date: 2021-08-26T01:23:52Z},
}

@misc{noauthor_googlepegasus-xsum_nodate,
	title = {google/pegasus-xsum · {Hugging} {Face}},
	url = {https://huggingface.co/google/pegasus-xsum},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-09-27},
	file = {Snapshot:/home/labicquette/Zotero/storage/YMNYT4WP/pegasus-xsum.html:text/html},
}

@misc{noauthor_pegasus_nodate,
	title = {Pegasus},
	url = {https://huggingface.co/docs/transformers/model_doc/pegasus},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-09-27},
	file = {Snapshot:/home/labicquette/Zotero/storage/5QKV24BI/pegasus.html:text/html},
}

@misc{noauthor_bart_nodate,
	title = {{BART}},
	url = {https://huggingface.co/docs/transformers/model_doc/bart},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-09-27},
}

@misc{noauthor_facebookbart-large-cnn_2024,
	title = {facebook/bart-large-cnn · {Hugging} {Face}},
	url = {https://huggingface.co/facebook/bart-large-cnn},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-09-27},
	month = jan,
	year = {2024},
	file = {Snapshot:/home/labicquette/Zotero/storage/PGBLC4S9/bart-large-cnn.html:text/html},
}

@misc{noauthor_text_nodate,
	title = {Text {Summarization} with {Bart} series {LLM}},
	url = {https://kaggle.com/code/aisuko/text-summarization-with-bart-series-llm},
	abstract = {Explore and run machine learning code with Kaggle Notebooks {\textbar} Using data from Samsum Dataset Text summarization},
	language = {en},
	urldate = {2024-09-27},
	file = {Snapshot:/home/labicquette/Zotero/storage/EWGUK9A6/text-summarization-with-bart-series-llm.html:text/html},
}

@misc{paul_pre-trained_2022,
	title = {Pre-trained {Language} {Models} for the {Legal} {Domain}: {A} {Case} {Study} on {Indian} {Law}},
	shorttitle = {Pre-trained {Language} {Models} for the {Legal} {Domain}},
	url = {https://arxiv.org/abs/2209.06049v5},
	abstract = {NLP in the legal domain has seen increasing success with the emergence of Transformer-based Pre-trained Language Models (PLMs) pre-trained on legal text. PLMs trained over European and US legal text are available publicly; however, legal text from other domains (countries), such as India, have a lot of distinguishing characteristics. With the rapidly increasing volume of Legal NLP applications in various countries, it has become necessary to pre-train such LMs over legal text of other countries as well. In this work, we attempt to investigate pre-training in the Indian legal domain. We re-train (continue pre-training) two popular legal PLMs, LegalBERT and CaseLawBERT, on Indian legal data, as well as train a model from scratch with a vocabulary based on Indian legal text. We apply these PLMs over three benchmark legal NLP tasks -- Legal Statute Identification from facts, Semantic Segmentation of Court Judgment Documents, and Court Appeal Judgment Prediction -- over both Indian and non-Indian (EU, UK) datasets. We observe that our approach not only enhances performance on the new domain (Indian texts) but also over the original domain (European and UK texts). We also conduct explainability experiments for a qualitative comparison of all these different PLMs.},
	language = {en},
	urldate = {2024-09-27},
	journal = {arXiv.org},
	author = {Paul, Shounak and Mandal, Arpan and Goyal, Pawan and Ghosh, Saptarshi},
	month = sep,
	year = {2022},
	file = {Full Text PDF:/home/labicquette/Zotero/storage/2YNPXRWR/Paul et al. - 2022 - Pre-trained Language Models for the Legal Domain A Case Study on Indian Law.pdf:application/pdf},
}

@misc{zheng_when_2021-1,
	title = {When {Does} {Pretraining} {Help}? {Assessing} {Self}-{Supervised} {Learning} for {Law} and the {CaseHOLD} {Dataset}},
	shorttitle = {When {Does} {Pretraining} {Help}?},
	url = {https://arxiv.org/abs/2104.08671v3},
	abstract = {While self-supervised learning has made rapid advances in natural language processing, it remains unclear when researchers should engage in resource-intensive domain-specific pretraining (domain pretraining). The law, puzzlingly, has yielded few documented instances of substantial gains to domain pretraining in spite of the fact that legal language is widely seen to be unique. We hypothesize that these existing results stem from the fact that existing legal NLP tasks are too easy and fail to meet conditions for when domain pretraining can help. To address this, we first present CaseHOLD (Case Holdings On Legal Decisions), a new dataset comprised of over 53,000+ multiple choice questions to identify the relevant holding of a cited case. This dataset presents a fundamental task to lawyers and is both legally meaningful and difficult from an NLP perspective (F1 of 0.4 with a BiLSTM baseline). Second, we assess performance gains on CaseHOLD and existing legal NLP datasets. While a Transformer architecture (BERT) pretrained on a general corpus (Google Books and Wikipedia) improves performance, domain pretraining (using corpus of approximately 3.5M decisions across all courts in the U.S. that is larger than BERT's) with a custom legal vocabulary exhibits the most substantial performance gains with CaseHOLD (gain of 7.2\% on F1, representing a 12\% improvement on BERT) and consistent performance gains across two other legal tasks. Third, we show that domain pretraining may be warranted when the task exhibits sufficient similarity to the pretraining corpus: the level of performance increase in three legal tasks was directly tied to the domain specificity of the task. Our findings inform when researchers should engage resource-intensive pretraining and show that Transformer-based architectures, too, learn embeddings suggestive of distinct legal language.},
	language = {en},
	urldate = {2024-09-27},
	journal = {arXiv.org},
	author = {Zheng, Lucia and Guha, Neel and Anderson, Brandon R. and Henderson, Peter and Ho, Daniel E.},
	month = apr,
	year = {2021},
	file = {Full Text PDF:/home/labicquette/Zotero/storage/8MTINB53/Zheng et al. - 2021 - When Does Pretraining Help Assessing Self-Supervised Learning for Law and the CaseHOLD Dataset.pdf:application/pdf},
}

@inproceedings{polsley_casesummarizer_2016-1,
	address = {Osaka, Japan},
	title = {{CaseSummarizer}: {A} {System} for {Automated} {Summarization} of {Legal} {Texts}},
	shorttitle = {{CaseSummarizer}},
	url = {https://aclanthology.org/C16-2054},
	abstract = {Attorneys, judges, and others in the justice system are constantly surrounded by large amounts of legal text, which can be difficult to manage across many cases. We present CaseSummarizer, a tool for automated text summarization of legal documents which uses standard summary methods based on word frequency augmented with additional domain-specific knowledge. Summaries are then provided through an informative interface with abbreviations, significance heat maps, and other flexible controls. It is evaluated using ROUGE and human scoring against several other summarization systems, including summary text and feedback provided by domain experts.},
	urldate = {2024-09-27},
	booktitle = {Proceedings of {COLING} 2016, the 26th {International} {Conference} on {Computational} {Linguistics}: {System} {Demonstrations}},
	publisher = {The COLING 2016 Organizing Committee},
	author = {Polsley, Seth and Jhunjhunwala, Pooja and Huang, Ruihong},
	editor = {Watanabe, Hideo},
	month = dec,
	year = {2016},
	pages = {258--262},
	file = {Full Text PDF:/home/labicquette/Zotero/storage/VFQI6Z9F/Polsley et al. - 2016 - CaseSummarizer A System for Automated Summarization of Legal Texts.pdf:application/pdf},
}

@inproceedings{nguyen_keyword-based_2023,
	address = {New York, NY, USA},
	series = {{ICAIL} '23},
	title = {Keyword-based {Augmentation} {Method} to {Enhance} {Abstractive} {Summarization} for {Legal} {Documents}},
	isbn = {9798400701979},
	url = {https://dl.acm.org/doi/10.1145/3594536.3595120},
	doi = {10.1145/3594536.3595120},
	abstract = {Since state-of-the-art machine learning models like Transformers can not handle long text well, the quality of the summarization of legal documents is still not desirable. In order to improve the ability of machine learning models to understand the context of a long document, we introduce the keywords into the models to guide the summarization to locate and capture key information from long documents such as legal cases. Different from other works leveraging keywords to enhance the model, we further investigate how keyword quality impacts summarization. To improve the performance of the summarization, we also explore different methods for effectively encoding exceptionally lengthy documents and models for keyword extraction. The experiment results demonstrated that the keywords-based augmentation method is effective for summarization and higher-quality keywords can enhance the summarization models.},
	urldate = {2024-10-01},
	booktitle = {Proceedings of the {Nineteenth} {International} {Conference} on {Artificial} {Intelligence} and {Law}},
	publisher = {Association for Computing Machinery},
	author = {Nguyen, Huyen and Ding, Junhua},
	month = sep,
	year = {2023},
	pages = {437--441},
	file = {Full Text PDF:/home/labicquette/Zotero/storage/SDJ938Q3/Nguyen et Ding - 2023 - Keyword-based Augmentation Method to Enhance Abstractive Summarization for Legal Documents.pdf:application/pdf},
}

@inproceedings{garimella_text_2022,
	address = {Abu Dhabi, United Arab Emirates (Hybrid)},
	title = {Text {Simplification} for {Legal} {Domain}: {Insights} and {Challenges}},
	shorttitle = {Text {Simplification} for {Legal} {Domain}},
	url = {https://aclanthology.org/2022.nllp-1.28},
	doi = {10.18653/v1/2022.nllp-1.28},
	abstract = {Legal documents such as contracts contain complex and domain-specific jargons, long and nested sentences, and often present with several details that may be difficult to understand for laypeople without domain expertise. In this paper, we explore the problem of text simplification (TS) in legal domain. The main challenge to this is the lack of availability of complex-simple parallel datasets for the legal domain. We investigate some of the existing datasets, methods, and metrics in the TS literature for simplifying legal texts, and perform human evaluation to analyze the gaps. We present some of the challenges involved, and outline a few open questions that need to be addressed for future research in this direction.},
	urldate = {2024-10-01},
	booktitle = {Proceedings of the {Natural} {Legal} {Language} {Processing} {Workshop} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Garimella, Aparna and Sancheti, Abhilasha and Aggarwal, Vinay and Ganesh, Ananya and Chhaya, Niyati and Kambhatla, Nandakishore},
	editor = {Aletras, Nikolaos and Chalkidis, Ilias and Barrett, Leslie and Goan{\textbackslash}textcommabelowtă, Cătălina and Preo{\textbackslash}textcommabelowtiuc-Pietro, Daniel},
	month = dec,
	year = {2022},
	pages = {296--304},
	file = {Full Text PDF:/home/labicquette/Zotero/storage/J498JZ9H/Garimella et al. - 2022 - Text Simplification for Legal Domain Insights and Challenges.pdf:application/pdf},
}

@misc{noauthor_early_nodate,
	title = {Early v. {Packer}, 537 {U}.{S}. 3 (2002)},
	url = {https://supreme.justia.com/cases/federal/us/537/3/},
	abstract = {Early v. Packer},
	language = {en},
	urldate = {2024-10-01},
	journal = {Justia Law},
	file = {Snapshot:/home/labicquette/Zotero/storage/AAS36KXB/3.html:text/html},
}

@misc{noauthor_campbell-ewald_nodate,
	title = {Campbell-{Ewald} v. {Gomez}, 577 {U}.{S}. 153 (2016)},
	url = {https://supreme.justia.com/cases/federal/us/577/153/},
	abstract = {Campbell-Ewald v. Gomez: A prior offer of judgment under Federal Rule of Civil Procedure 68 does not make a case moot if it has been rejected, since the parties are still adverse and the settlement proposal is no longer effective. Also, sovereign immunity does not apply to a federal contractor that exceeds the scope of its authority.},
	language = {en},
	urldate = {2024-10-01},
	journal = {Justia Law},
	file = {Snapshot:/home/labicquette/Zotero/storage/BDH3VLUZ/153.html:text/html},
}
